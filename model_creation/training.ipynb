{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "cc0aa35067407a9ad921f707db2afe276d86013966be43ca5de1bca1e9dafe5b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import T5TokenizerFast, TFT5ForConditionalGeneration\n",
    "from progressbar import progressbar as pb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('new_dataset.csv',index_col=0)\n",
    "data=data[['txt','PERSON','NORP','JOB','JOB1','GPE','ORG','TENSE']]\n",
    "data['TENSE'] = np.where((data['TENSE'] == 'was'),'PAST','PRESENT')\n",
    "data=shuffle(data)\n",
    "\n",
    "n = 30000\n",
    "df_train = data.iloc[:n,1:]\n",
    "df_label = data.iloc[:n,0:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-13 19:41:54.879000: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-07-13 19:41:54.879282: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-07-13 19:41:54.879416: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (HAL-9000): /proc/driver/nvidia/version does not exist\n",
      "2021-07-13 19:41:54.882593: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-13 19:41:54.949502: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-small\")\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "num_of_batches=math.floor(len(df_train)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=[df_label.values[i][0] for i in range (len(df_label))]\n",
    "label_encoded=tokenizer.batch_encode_plus(label,padding=True,max_length=900,return_tensors='tf')[\"input_ids\"]\n",
    "\n",
    "inputbatch=['|'.join(df_train.iloc[i].dropna().values.tolist()) for i in range (len(df_train))]\n",
    "input_encoded=tokenizer.batch_encode_plus(inputbatch,padding=True,max_length=900,return_tensors='tf')[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "N/A% (0 of 3) |                          | Elapsed Time: 0:00:00 ETA:  --:--:--\n",
      "2021-07-13 19:30:10.281597: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 168607744 exceeds 10% of free system memory.\n",
      "2021-07-13 19:30:11.903808: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 168607744 exceeds 10% of free system memory.\n",
      "2021-07-13 19:30:12.404566: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 168607744 exceeds 10% of free system memory.\n",
      "2021-07-13 19:30:14.384396: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 168607744 exceeds 10% of free system memory.\n",
      "2021-07-13 19:30:14.858102: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 168607744 exceeds 10% of free system memory.\n",
      " 33% (1 of 3) |########                  | Elapsed Time: 0:00:15 ETA:   0:00:31\n",
      " 66% (2 of 3) |#################         | Elapsed Time: 0:00:30 ETA:   0:00:14\n",
      "100% (3 of 3) |##########################| Elapsed Time: 0:01:07 Time:  0:01:07\n"
     ]
    }
   ],
   "source": [
    "epoch=1\n",
    "for k in range(epoch):\n",
    "\n",
    "    running_loss=0\n",
    "    \n",
    "    \n",
    "    for i in pb(range(num_of_batches)):\n",
    "        input=input_encoded[i*batch_size:i*batch_size+batch_size]\n",
    "        output=label_encoded[i*batch_size:i*batch_size+batch_size]\n",
    "        \n",
    "        #Entrainement sur le batch\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = model(input_ids=input, labels=output)\n",
    "            # Compute the loss value for this batch.\n",
    "            loss_value = sum(outputs.loss)/len(outputs.loss)\n",
    "            running_loss+=loss_value\n",
    "        # Actualisation des poids\n",
    "        gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}