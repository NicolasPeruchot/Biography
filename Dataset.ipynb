{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd02c389fa4cf50b850ad874e3b9b3165a376e69045109ccdeae25cb1aa36938185",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "ca3246a3c8e83b87b3df0b73736644c8280d5b382a6c9272defde1b8977a365a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from icecream import ic\n",
    "import spacy\n",
    "import progressbar\n",
    "from spacy.matcher import Matcher, DependencyMatcher\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('dataframe_true_final.csv')\n",
    "text=data['txt']\n",
    "data=data.drop(columns=['Unnamed: 0','txt','id'])\n",
    "text=text.str.replace(\"'\",\"\",regex=False)\n",
    "text=text.str.replace(\"]\",\"\",regex=False)\n",
    "text=text.str.replace(\"[\",\"\",regex=False)\n",
    "text=text.str.replace(\",\",\"\",regex=False)\n",
    "text=text.str.replace(\"A \",\"a \",regex=False)\n",
    "text=text.str.replace(\"And\",\"and\",regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata=pd.DataFrame(index=text.index, columns=['PERSON','JOB','JOB1','NORP','GPE','ORG','TENSE'])\n",
    "\n",
    "matcher_nom=DependencyMatcher(nlp.vocab)\n",
    "pattern_nom=[[{\"RIGHT_ID\":\"nom\",\"RIGHT_ATTRS\":{\"DEP\":\"nsubj\"}},{\"LEFT_ID\":\"nom\",\"REL_OP\":\">>\",\"RIGHT_ID\":\"reste\",\"RIGHT_ATTRS\":{}}]]\n",
    "matcher_nom.add(\"nom\",pattern_nom)\n",
    "\n",
    "matcher_job = Matcher(nlp.vocab)\n",
    "pattern_job = [[{\"DEP\": \"attr\"}]]\n",
    "matcher_job.add(\"job\", pattern_job)\n",
    "\n",
    "matcher_job1=DependencyMatcher(nlp.vocab)\n",
    "pattern_job1=[[{\"RIGHT_ID\":\"nom\",\"RIGHT_ATTRS\":{\"DEP\":\"attr\"}},{\"LEFT_ID\":\"nom\",\"REL_OP\":\">\",\"RIGHT_ID\":\"reste\",\"RIGHT_ATTRS\":{\"DEP\":\"compound\"}}]]\n",
    "matcher_job1.add(\"job1\",pattern_job1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100% (100 of 100) |######################| Elapsed Time: 0:00:01 Time:  0:00:01\n"
     ]
    }
   ],
   "source": [
    "wrong=set()\n",
    "for i in progressbar.progressbar(range (100)):\n",
    "  doc=nlp(text[i])\n",
    "  A=matcher_nom(doc)\n",
    "  B=matcher_job(doc)\n",
    "  C=matcher_job1(doc)\n",
    "  if A!=[]:   \n",
    "    newdata.loc[i]['PERSON']=doc[A[0][1][1]:A[0][1][0]+1].text\n",
    "  else: \n",
    "    wrong.add(i)\n",
    "  if B!=[]:   \n",
    "    newdata.loc[i]['JOB']=doc[B[0][1]:B[0][2]].text\n",
    "  else: \n",
    "    wrong.add(i)\n",
    "  if C!=[]:\n",
    "    Span=sorted(C[0][1])\n",
    "    Job=doc[Span[0]:Span[1]]\n",
    "    if len(Job.ents)!=0:\n",
    "\n",
    "      if Job.ents[0].label_=='NORP':\n",
    "        Job=Job[1:]\n",
    "    if len(Job)!=0:\n",
    "      newdata.loc[i]['JOB1']=Job.text\n",
    "\n",
    "\n",
    "  for col in ['NORP','GPE','ORG']:\n",
    "    for ent in doc.ents:\n",
    "      if ent.label_==col and newdata.iloc[i][col]!=newdata.iloc[i][col]:\n",
    "        newdata.loc[i][col]=ent.text\n",
    "  try:\n",
    "    if doc[matcher_verb(doc)[0][1]].morph.get(\"Tense\")[0]=='Pres':\n",
    "      newdata.loc[i]['TENSE']='is'\n",
    "    else:\n",
    "      newdata.loc[i]['TENSE']='was'\n",
    "  except:\n",
    "    wrong.add(i)\n",
    "\n",
    "\n",
    "\n",
    "for df in [newdata, text]:\n",
    "  df.drop(wrong).reset_index().drop(columns=['index'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def essai(i): #tester le nouveau dataset\n",
    "  ic(text[i])\n",
    "  ic(newdata.iloc[i])\n",
    "  doc=nlp(text[i])\n",
    "  for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.morph)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([text,newdata],axis=1).to_csv('New_dataset.csv')"
   ]
  }
 ]
}