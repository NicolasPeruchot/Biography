{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd02c389fa4cf50b850ad874e3b9b3165a376e69045109ccdeae25cb1aa36938185",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "ca3246a3c8e83b87b3df0b73736644c8280d5b382a6c9272defde1b8977a365a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from icecream import ic\n",
    "import spacy\n",
    "import progressbar\n",
    "from spacy.matcher import Matcher, DependencyMatcher\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('dataframe_true_final.csv')\n",
    "text=data['txt']\n",
    "data=data.drop(columns=['Unnamed: 0','txt','id'])\n",
    "text=text.str.replace(\"'\",\"\",regex=False)\n",
    "text=text.str.replace(\"]\",\"\",regex=False)\n",
    "text=text.str.replace(\"[\",\"\",regex=False)\n",
    "text=text.str.replace(\",\",\"\",regex=False)\n",
    "text=text.str.replace(\"A \",\"a \",regex=False)\n",
    "text=text.str.replace(\"And\",\"and\",regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata=pd.DataFrame(index=text.index, columns=['PERSON','JOB','NORP','GPE','ORG'])\n",
    "\n",
    "matcher_nom=DependencyMatcher(nlp.vocab)\n",
    "pattern_nom=[[{\"RIGHT_ID\":\"nom\",\"RIGHT_ATTRS\":{\"DEP\":\"nsubj\"}},{\"LEFT_ID\":\"nom\",\"REL_OP\":\">\",\"RIGHT_ID\":\"reste\",\"RIGHT_ATTRS\":{}}]]\n",
    "matcher_nom.add(\"nom\",pattern_nom)\n",
    "\n",
    "matcher_job = Matcher(nlp.vocab)\n",
    "pattern_job = [[{\"DEP\": \"attr\"}]]\n",
    "matcher_job.add(\"JOB\", pattern_job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100% (100 of 100) |######################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "wrong=set()\n",
    "for i in progressbar.progressbar(range (100)):\n",
    "  doc=nlp(text[i])\n",
    "  A=matcher_nom(doc)\n",
    "  B=matcher_job(doc)\n",
    "  if A!=[]:   \n",
    "    newdata.loc[i]['PERSON']=doc[A[0][1][1]:A[0][1][0]+1].text\n",
    "  else: \n",
    "    wrong.add(i)\n",
    "  if B!=[]:   \n",
    "    newdata.loc[i]['JOB']=doc[B[0][1]:B[0][2]].text\n",
    "  else: \n",
    "    wrong.add(i)\n",
    "\n",
    "\n",
    "  for col in ['NORP','GPE','ORG']:\n",
    "    for ent in doc.ents:\n",
    "      if ent.label_==col and newdata.iloc[i][col]!=newdata.iloc[i][col]:\n",
    "        newdata.loc[i][col]=ent.text\n",
    "\n",
    "\n",
    "\n",
    "for df in [newdata, text]:\n",
    "  df.drop(wrong).reset_index().drop(columns=['index'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def essai(i): #tester le nouveau dataset\n",
    "  ic(text[i])\n",
    "  doc=nlp(text[i])\n",
    "  for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ic| text[i]: 'Heiko Engelkes was a German journalist'\n",
      "Heiko PROPN compound Engelkes\n",
      "Engelkes NOUN nsubj was\n",
      "was AUX ROOT was\n",
      "a DET det journalist\n",
      "German ADJ amod journalist\n",
      "journalist NOUN attr was\n"
     ]
    }
   ],
   "source": [
    "essai(130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([text,newdata],axis=1).to_csv('New_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher_job=DependencyMatcher(nlp.vocab)\n",
    "pattern_job=[[{\"RIGHT_ID\":\"nom\",\"RIGHT_ATTRS\":{\"DEP\":\"attr\"}},{\"LEFT_ID\":\"nom\",\"REL_OP\":\">>\",\"RIGHT_ID\":\"reste\",\"RIGHT_ATTRS\":{\"DEP\":{\"IN\": [\"amod\", \"compound\"]}}}]]\n",
    "matcher_job.add(\"nom\",pattern_job)\n",
    "\n",
    "matcher_attr = Matcher(nlp.vocab)\n",
    "pattern_attr = [[{\"DEP\": \"attr\"}]]\n",
    "matcher_attr.add(\"JOB\", pattern_attr)\n",
    "\n",
    "# matcher_job=DependencyMatcher(nlp.vocab)\n",
    "# pattern_job=[[{\"RIGHT_ID\":\"gauche\",\"RIGHT_ATTRS\":{}},{\"LEFT_ID\":\"gauche\",\"REL_OP\":\"<<\",\"RIGHT_ID\":\"job\",\"RIGHT_ATTRS\":{\"DEP\":\"attr\"}},{\"LEFT_ID\":\"job\",\"REL_OP\":\">\",\"RIGHT_ID\":\"reste\",\"RIGHT_ATTRS\":{}}]]\n",
    "# matcher_job.add(\"nom\",pattern_job)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ic| doc: Hans Behrendt was a German Jewish actor screenwriter and film director\n",
      "ic| x[1]: [10, 4]\n",
      "ic| y: [4, 10]\n",
      "ic| doc[y[0]:y[1]]: German Jewish actor screenwriter and film\n",
      "ic| x[1]: [10, 5]\n",
      "ic| y: [5, 10]\n",
      "ic| doc[y[0]:y[1]]: Jewish actor screenwriter and film\n",
      "ic| x[1]: [10, 6]\n",
      "ic| y: [6, 10]\n",
      "ic| doc[y[0]:y[1]]: actor screenwriter and film\n",
      "ic| doc[B[0][1]:B[0][2]]: director\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "director"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "doc=nlp(text[165])\n",
    "ic(doc)\n",
    "A=matcher_job(doc)\n",
    "B=matcher_attr(doc)\n",
    "for x in A:\n",
    "    ic(x[1])\n",
    "    y=sorted(x[1])\n",
    "    ic(y)\n",
    "    ic(doc[y[0]:y[1]])\n",
    "ic(doc[B[0][1]:B[0][2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ic| j: 0\n",
      "ic| Z[j:j+1]: German\n",
      "ic| j: 0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 167
    }
   ],
   "source": [
    "\n",
    "# Z=doc[i[1][1]:i[1][0]+1]\n",
    "# for j in range(len(Z)):\n",
    "#     ic(j)\n",
    "#     ic(Z[j:j+1])\n",
    "#     if Z[j:j+1].ents!=[]:\n",
    "#         if Z[j:j+1].ents[0].label_=='NORP':\n",
    "#             break\n",
    "\n",
    "# ic(j)"
   ]
  }
 ]
}